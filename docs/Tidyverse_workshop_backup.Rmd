---
title: "Tidyverse101"
author: "Stijn Van de Vondel"
date: "27-7-2021"
output: 
  rmdformats::robobook:
    toc_depth: 3
---


```{r setup, include=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="120")
opts_chunk$set(#echo = FALSE,
	             #cache = TRUE,
               #prompt = FALSE,
               #tidy = TRUE,
               comment = NA#,
               #message = FALSE,
               #warning = FALSE
  )
opts_knit$set(width = 120)

```

```{r colorize function, include = FALSE}
  # colour text
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```
## 0. Prerequisites
### 0.1 Target audience
This course is targeted at Bachelor's, Master's and PhD students with a basic understanding of the R programming language. If you *can* answer the following questions without much of a hassle, please move on to **[0.2 Preparations]**:

* What does the `<-` operator do?
* How to create a vector with the values `"this"`, `"is"`, `"a"`, `"vector"`?
* How to access R's built-in help files (also referred to as "R documentation") for the function `shapiro.test`?
* Why *doesn't* the following piece of code work: `plot(foobar)`?
* How to install and load a new package into your R session?

In case you had trouble with *any* of these questions, please take some time to get comfortable with some R basics. As we have a lot of ground to cover, it would be unwise to jump in unprepared!
I strongly recommend the `swirl` package, which interactively introduces you to R (see [**swirl's website**](https://swirlstats.com/students.html) for more information). To get started with `swirl` right away, install the package using `install.packages("swirl")`, load it into your session with `library(swirl)` and jump-start your journey with `swirl()`. Going through the first chapter (`1: Basic Building Blocks`) should suffice, but don't let that stop you from learning more R!

### 0.2 Preparations
**Please go through all of the steps below before attending the workshop!!**

Make sure to have at least `R version 4.1.0` installed on your computer. Additionally, I strongly recommend installing Rstudio, as I will be using this as my Integrated Development Environment (IDE) throughout this course.

* Installing R: [https://www.r-project.org/](https://www.r-project.org/)
* Installing RStudio: [https://www.rstudio.com/products/rstudio/download/#download](https://www.rstudio.com/products/rstudio/download/#download)

Finally, install tidyverse using `install.packages("tidyverse")`.

## 1. Introduction
### 1.1 Context
As a biology student, I was introduced to R in the very first year of the programme. With R being my first scripting language, it was as much an uphill struggle as any other new language. In the second year, R was thrown on the table again in the context of statistics, with another round of RStats in the Master programme, 2 years later.
In this time, I used R only used as a means to perform statistical tests. As real, raw data was rarely in the format that was presented during any of the statistical courses, I cleaned, filtered, pivoted, ... all of it using MS Excel. If you haven't already, this can be a very time- and energy-consuming endeavour! Indeed, we never really learned how to clean and wrangle our datasets, leading to a lot of trouble and frustration toward data analysis.

During my thesis, however, I found out about 'Tidyverse', but never *truly* submerged myself. At the start of my PhD in October 2020, I seized the moment to learn the ropes of this package, and I haven't looked back since. To potentially save you a lot of time and trouble - whether you are a Bachelor, Master or PhD student, or perhaps even something beyond that -, I want to share with you some of the things I have learned along the way. For the record, I'm far from an expert on the matter, and there is still a lot left to explore! 

This short origin story aside, hopefully this material will prove to be helpful somewhere along your data journey. There are many ways to deal with data tidying and wrangling, and the `tidyverse` just happens to be the one I prefer at the moment. Feel free to send me any and all feedback you may have to Stijn.VandeVondel@uantwerpen.be.

### 1.2 Tidyverse
The [tidyverse](https://www.tidyverse.org/ "Tidyverse.org") is "*an opinionated collection of R packages designed for data science*". In that sense, `tidyverse` can be represented as a virtual basket containing different packages, which "*all share an underlying design philosophy, grammar, and data structures*". In other words, these packages and their corresponding functions easily interact with each other, allowing for a wide range of tools to tinker with data.

If you haven't already, install the `tidyverse` (`install.packages("tidyverse")`) and load it into your R environment.
```{r load tidyverse, collapse = TRUE, warnings = FALSE}
  # load tidyverse
library(tidyverse)

```
As shown above, a series of packages will be installed. Each of these packages is listed below, along with a brief description borrowed from the packages' documentation. In addition to these 'core' packages, other R libraries are also installed along with `tidyverse`, but are mostly beyond the scope of this workshop.

* [**ggplot2**](https://ggplot2.tidyverse.org/): A system for 'declaratively' creating graphics, based on "The Grammar of Graphics".
* [**dplyr**](https://dplyr.tidyverse.org/): `dplyr` provides a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges.
* [**tidyr**](https://tidyr.tidyverse.org/): `tidyr` provides a set of functions that help you get to tidy data. Tidy data is data with a consistent form: in brief, every variable goes in a column, and every column is a variable. This is part of the *core philosophy of tidy data*.
* [**readr**](https://readr.tidyverse.org/): `readr` provides a fast and friendly way to read rectangular data (like csv, tsv, and fwf). It is designed to flexibly parse many types of data found in the wild, while still cleanly failing when data unexpectedly changes.
* [**purrr**](https://purrr.tidyverse.org/): `purrr` enhances R’s functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. Once you master the basic concepts, purrr allows you to replace many for loops with code that is easier to write and more expressive.
* [**tibble**](https://tibble.tidyverse.org/): `tibble` is a modern re-imagining of the data frame, keeping what time has proven to be effective, and throwing out what it has not. Tibbles are data.frames that are lazy and surly: they do less and complain more forcing you to confront problems earlier, typically leading to cleaner, more expressive code. 
* [**stringr**](https://stringr.tidyverse.org/): `stringr` provides a cohesive set of functions designed to make working with strings as easy as possible. It is built on top of `stringi`, which uses the ICU C library to provide fast, correct implementations of common string manipulations.
* [**forcats**](https://forcats.tidyverse.org/): `forcats` provides a suite of useful tools that solve common problems with factors. R uses factors to handle categorical variables, variables that have a fixed and known set of possible values.
* Others: `broom`, `cli`, `crayon`, `dbplyr`, `dtplyr`, `googledrive`, `googlesheets4`, `haven`, `hms`, `httr`, `jsonlite`, `lubridate`, `magrittr`, `modelr`, `pillar`, `readxl`, `reprex`, `rlang`, `rstudioapi`, `rvest`, `xml2`

If you have already installed `tidyverse` earlier, you may want to check whether all packages contained within are up-to-date.
```{r update tidyverse, eval = FALSE}
  # check for updates
tidyverse::tidyverse_update()

```
If a package is out of date, you will receive a notification and instructions to update outdated packages.

```{r, echo=FALSE, out.width="60%", fig.cap = "Try `tidyverse_packages(include_self = TRUE)` and see for yourself!", fig.align="center", fig.retina = 2}
knitr::include_graphics("images/tidyverse_meme.png")
```

## 2. The whole game
### 2.1 In a nutshell
>“Tidy datasets are all alike, but every messy dataset is messy in its own way.” 
`r tufte::quote_footer("Hadley Wickham, a play on Tolstoy's “Happy families are all alike; every unhappy family is unhappy in its own way.”")`

**More often than not, raw data will not be formatted in a very accessible, analysis-friendly way.** On the contrary, experiments generally do not produce clean *trees.csv* or *covid_cases.csv* files, but datasets that are wild, exotic or downright savage. This is especially likely if someone else collected data *for* you, but did not have any prior knowledge about your general set-up. If you cannot recall hours of tedious data tinkering in Excel following a group lab practical, then have you really lived your student life to the fullest? :-)

```{r, echo=FALSE, out.width="60%", fig.cap = "If only Robbie would stop bothering Alexa...            Source: [Jon Carter](https://www.kdnuggets.com/2017/09/cartoon-machine-learning-class.html)", fig.align="center"}
knitr::include_graphics("images/datacleaning_meme.jpg")
```

To deal with such datasets, rigorous cleaning and wrangling is required before even thinking about modelling or visualizing the story residing within your data. The schematic below (as shown in [R for Data Science](https://r4ds.had.co.nz/), which is also great reference material!) encompasses the entire process of data science with `tidyverse`; this course is limited to the parts **`r colorize("highlighted in blue", "#169EFC")`**. Most importantly, **data tidying** and **transformation** will be taking centre stage, with some notes on **importing data** and dipping our toes in **visualization**.

```{r, echo=FALSE, out.width="70%", fig.cap = "Data wrangling. Source: [R for Data Science](https://r4ds.had.co.nz/wrangle-intro.html)", fig.align="center"}
knitr::include_graphics("images/data-science-wrangle.png")
```

### 2.2 Data cleaning vs data wrangling
The above schematic in words: Raw data first needs to be **imported** by loading it into the R environment (which *usually* means effectively loading data into memory (RAM)). Once loaded, data often requires **tidying** (or *data cleaning*) and **transformation** (or *data wrangling*) prior to any analyses down the line. In more exact terms, **data cleaning** is the **process through which errors are fixed and data quality is ensured**, while **data wrangling** would be defined as the **process through which raw data is manipulated and transformed**. As tools for both processes can often be used interchangeably, I will not distinguish between these definitions. For the sake of everyone's convenience, I will simply talk about **data wrangling** as a whole.

```{r backup, echo = FALSE}
#In more exact terms, **data cleaning** is the **process through which error are fixed and data quality is ensured**. For instance, data may be spread across multiple #columns (e.g. surveys where each row is a question, and each column is the answer of a specific age group), may contain a lot of missing data or data you don't need #(e.g. trying to answer questions at the scale of Belgium, while your data contains multiple countries) or simply data that is downright dirty (e.g. inconsistent values #because of typos). Needless to say, as Wickham put it, "every dataset is messy in its own way", so each dataset will likely require a different approach.
#
#The act of **data wrangling** *sensu stricto* is the **process through which raw data is manipulated and transformed**. We may be interested in calculating indices #based on raw measurement data, or perhaps we want to calculate an average per category. In other cases, we may want to join multiple tables together, arrange and #rename columns, or remove them altogether.
```

## 3. Tibbles and pipes
Before diving head first into the `tidyverse`, we will need to talk about **tibbles** and **pipes.** Very many functions within the `tidyverse` produce tibbles, making it at least worth mentioning. Pipes, on the other hand, are a powerful tool for clearly expressing a sequence of multiple operations. 

### 3.1 Tibbles
**Tibbles are dataframes**, but with a twist. For the sake of comparison and clarification, we will create a classical R data frame and a tibble with the same content.

```{r tibbles, collapse = TRUE}
  # set seed for reproducibility
set.seed(1)
  
  # create a dataframe
a_dataframe <- data.frame(x = 1:25,
           y = rnorm(25, 1, 2))

  # create a tibble from the dataframe
a_tibble <- as_tibble(a_dataframe)

```

Here, we have created two datasets, each containing the same information: column `x` with numbers from 1 to 25, and column `y` with 25 random observations drawn from a normal distribution. Both have the same number of variables (2) and observations (25), and will produce the same results (try running e.g. `all.equal(mean(a_dataframe$y), mean(a_tibble$y))`). This begs the question of *what*, exactly, is different?

One hint toward the answer can be obtained by running both objects (simply `a_dataframe` and `a_tibble` in the console) and reviewing the output.

```{r, echo = FALSE, out.width="60%", fig.cap = "A regular dataframe (left) and a tibble (right). The tibble shows a couple of distinct features to improve printing and inspection of your data.", fig.align="center"}
knitr::include_graphics("images/dataframe_vs_tibble.png")
```

As shown in the figure above, a couple of features are shown in a tibble that are non-existent for 'regular' dataframes. In a way, a tibble is nothing more than a data frame with some extra 'quality of life' features. However, there are other important differences going on under the hood, encapsulating best practices for data frames. Read more on tibbles [here](https://tibble.tidyverse.org/reference/tbl_df-class.html).

### 3.2a Pipes: short version
Pipes are a special operator aimed at making code more intuitive to read and write (though opinions may differ). The `tidyverse` pipe is written as ` %>% ` (`CTRL+SHIFT+M` in RStudio) and originates from the `magrittr` package ([to be pronounced with a sophisticated french accent](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html)). They are automatically loaded in with `library(tidyverse)`, but can also be loaded separately using `library(magrittr)` or `library(magrittr, include.only = "%>%")`. 

In brief, the `magrittr` pipe passes the output of what comes *before* the pipe (left-hand side) as input to the function *after* the pipe (right-hand side). The pseudo-code below shows what this looks like in the context of baking cookies in a factory, going through the functions and pipes as if it were a conveyer belt.

```{r yummie cookies A, eval=FALSE}
raw_ingredients <- ("butter", "sugar", "eggs", "chocolate chips", "...")

choc_chip_cookies <- raw_ingredients %>% 
  make_dough() %>% 
  shape_cookie() %>% 
  transport_to_oven() %>% 
  bake_yummie_cookies() %>% 
  cool_cookies() %>% 
  pack() %>% 
  send_away()
  
```
In case you don't completely understand the ` %>% ` yet, I've written a more lengthy section below (**[3.2b Pipes: long version]**). It will help you to better grasp the benefits of the operator, but is not required to get you going with `tidyverse`.

NOTE: Ever since `R version 4.1.0`, R also sports a native pipe operator `|>`, but this will not be covered in this course. 

### 3.2b Pipes: long version
To give you a working example, I will provide some code (see below) with one of most used built-in R datasets: `mtcars`. Don't worry too much about the different functions used (we will get back to most of those later!), but pay attention to the use of the ` %>% ` operator in example 1.

```{r piping}
  # example 1: piping
df_cars1 <- mtcars %>% 
  rownames_to_column("car") %>% 
  filter(str_detect(car, "Merc")) %>% 
    # convert miles per gallon -> km per liter
  mutate(kml = mpg*(1.60934/3.78541)) %>% 
  select(car, kml, cyl, hp)
```

As the benefit of the ` %>% ` will not immediately become clear after this example, consider the following code blocks (examples 2 to 4):
```{r no piping1}
  # example 2: nesting
df_cars <- select(
  mutate(
    filter(rownames_to_column(mtcars, "car"), str_detect(car, "Merc")),
    kml = mpg*(1.60934/3.78541)), 
  car, kml, cyl, hp)
```
```{r no piping2}
  # example 3: overwriting
df_cars <- rownames_to_column(mtcars, "car")
df_cars <- filter(df_cars, str_detect(car, "Merc"))
df_cars <- mutate(df_cars, kml = mpg*(1.60934/3.78541))
df_cars <- select(df_cars, car, kml, cyl, hp)
```
```{r base R}
  # example 4: base R
df_cars <- mtcars
df_cars$car <- row.names(df_cars)
df_cars <- df_cars[, c(ncol(df_cars), 1:ncol(df_cars)-1)]
df_cars <- df_cars[grep("Merc", df_cars$car), ]
row.names(df_cars) <- NULL
df_cars$kml <- df_cars$mpg*(1.60934/3.78541)
df_cars <- df_cars[c("car", "kml", "cyl", "hp")]

```

**All of the above examples will yield the same `df_cars` object at the very end.** In my humble opinion, **example 1** is the most readable code *by far* (but, again, mileage and opinions may vary). Once you become used to piping multiple operations together into one chain, you no longer need to intermediately save or overwrite old data (with some exceptions). Additionally, code becomes more intuitive and readable if used correctly.

Another thing you may (or may not) have noticed, is how in **example 1** (as well as in **example 2**, but for different reasons) `df_cars` is only mentioned *once*, while **example 3** and **4** mention the object `df_cars` **7** and **15**(!!) times, respectively. As for **example 2**, nesting all of the functions limited the number of mentions of `df_cars`, at the cost of readability. Naturally, one could also nest some of the operations in **example 4**, but I think we all have better things to do!

In any case, where is each function in example 1 getting their data from, or how does it know which one to use? And what is the order of execution of each of these function calls? 
To explain this, imagine a factory that produces your favourite type of cookie - I will go with the classic ol' chocolate chip. At one end, raw ingredients (butter, sugar, eggs, chocolate chips, ...) are delivered to the factory's doorstep. At the other end, the factory pumps out boxes chock-full of delicious cookies. 

Of course, we all know the factory isn't a [black box](https://www.merriam-webster.com/dictionary/black%20box), but rather an intricate system of many different steps. **First**, raw ingredients need to be mixed into a batter and thickened into a dough. **Next**, this dough is poured into moulding machines where the cookies are given their iconic shape. **Then**, the cookie-shaped dough moves down a conveyer belt to an industrial oven for baking. **Finally**, *after* cooling these heavenly cookies, they are packed and sent away. Let's write this into some pseudo-code using the `magrittr` pipe:
```{r yummie cookies B, eval=FALSE}
raw_ingredients <- ("butter", "sugar", "eggs", "chocolate chips", "...")

choc_chip_cookies <- raw_ingredients %>% 
  make_dough() %>% 
  shape_cookie() %>% 
  transport_to_oven() %>% 
  bake_yummie_cookies() %>% 
  cool_cookies() %>% 
  pack() %>% 
  send_away()
  
```

In case you hadn't noticed yet, **the pipe passes the output from the first line onto the next line as the input**. As such, `raw_ingredients` is passed on to `make_dough()`, and the result of `make_dough()` is passed on to `shape_cookie()`. Once the data has gone through `send_away()`, it is stored in the object called `choc_chip_cookies` - the finished box of cookies, if you will. In terms of the code above, you could also write each of the cookie-making steps on one line of code (like a virtual conveyer belt), but this would make the code much less readable.

Without going into too much detail, this behaviour is also engrained into most `tidyverse` functions. Most of these functions (also referred to as **verbs**), use the output of whatever comes *before* the pipe as the input for the verb *after* the pipe. If you want to explicitly refer to this input-output within these function calls, the dot (`.`) placeholder can be used: 

```{r yummie cookies2, eval=FALSE}
raw_ingredients <- ("butter", "sugar", "eggs", "chocolate chips", "...")

choc_chip_cookies <- raw_ingredients %>% 
  make_dough(.) %>% 
  shape_cookie(.) %>% 
  transport_to_oven(.) %>% 
  bake_yummie_cookies(.) %>% 
  cool_cookies(.) %>% 
  pack(.) %>% 
  send_away(.)
  
```

For now, this is all you need to know about pipes (and far more than I knew when I started out). In brief, the magrittr pipe passes the output of what comes *before* the pipe (left-hand side) as input to the function *after* the pipe (right-hand side). For more technical information, see [here](https://magrittr.tidyverse.org/reference/pipe.html). Note that, ever since `R version 4.1.0`, R also sports a native pipe operator `|>`, but this will not be covered in this course. 

```{r, echo = FALSE, out.width="50%", fig.cap = "*'Ceci n'est pas une pipe'*, by Belgian artist René Magritte, which served to be the etymological inspiration for the [*magrittr*](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html) package.", fig.align="center"}
knitr::include_graphics("images/MagrittePipe.jpg")
```

### 3.3 Tidy data
As already touched upon in the description of the `dplyr` package, tidy data is data with a consistent form and follows three rules:

* Each variable must have its own column.
* Each observation must have its own row.
* Each value must have its own cell.

```{r, echo = FALSE, out.width="70%", fig.cap = "The three rules of tidy data. Source: '[R for Data Science'](https://r4ds.had.co.nz/tidy-data.html)", fig.align="center"}
knitr::include_graphics("images/tidy_data.png")
```

If data is tidy, then **every variable goes in a column, and every column is a variable.** Tidy data is not desirable in *all* cases, but can prove to be a very robust way of structuring data when using `tidyverse`. For those who have already worked with `ggplot2` may know what I am talking about! For more more information and examples, head on over to [12.2 Tidy data in R4DS](https://r4ds.had.co.nz/tidy-data.html#tidy-data-1).

## 4. Importing
### 4.1 `readr` and base R
As this workshop is more about the actual data cleaning and wrangling, I will only go over **importing data** very briefly.

Within `tidyverse`, the `readr` package was developed to provide a fast and friendly way to read rectangular data (e.g. csv). The `readr` functions you're likely to use most often are:

* `read_csv()` to read comma (`,`) delimited files;
* `read_csv2()` to read semicolon (`;`) separated files (a common filetype in Belgium, where `,` is used as the decimal point);
* `read_tsv()` to read tab delimited files;
* `read_delim()` to read files with any delimiter.

As you may know, base R also has tools to import data (e.g. `read.csv()`, `read.csv2`, `read.table()`, ...), but these are generally **slower** than those provided by `readr`. Regardless, this is unlikely to be an issue unless you are working with larger datasets (1 million observations and up).

### 4.2 Big data
In case your workflow is suffering from large data files, fear not - there are powerful tools at your disposal! Introducing `vroom` and `data.table`. Both packages use multiple threading, which is very beneficial if your computer possesses multiple CPU cores (which is often the case, nowadays). This, along with some other nifty features, allows reading and writing data *very* fast (one could say *vrrroooom*). As opposed to `data.table`, `vroom` does not fully read data into memory, but only indexes it. This means that only columns and rows you would actually put to use would be read.

That said, `data.table` still ought to be faster in case of numeric data than `vroom`. On top of that, `data.table` provides tools and syntax to wrangle data much more efficiently than e.g. `tidyverse` functions. In addition, `data.table` is also faster and more memory-efficient in doing so, but its syntax is more difficult to read and write. As a compromise, `dtplyr` has been called into existence within the `tidyverse`, which uses the same 'tidy verbs' you'll become familiar with, but translates this into `data.table` syntax to benefit of its sheer speed (with some minor loss of speed due to [overhead](https://stackoverflow.com/questions/2860234/what-is-overhead/2860263), and loss of memory-efficiency). 
One additional thing I want to mention about `data.table` is its very convenient *and* fast way of reading (`fread`) and writing (`fwrite`) data with. It is highly similar to base R's `read.table()`, but automatically detects column separators, data types, and has many arguments to customize the function call.

Nevertheless, as of [`readr 2.0.0`](https://www.tidyverse.org/blog/2021/07/readr-2-0-0/), the package uses `vroom` as a backend, granting an impressive speed boost. Below, I provide a benchmark comparing base R's `read.csv()`, the previous (*.old*) and current version of `read_csv()` (*.new*), as well as `data.table::fread()` to import a `.csv` file containing 16 columns of > 34 million rows, mostly numerical data (total size: 3.56 GB, uncompressed). 

```{r comparison code, eval = FALSE, echo = FALSE}
  # define file path
path <- "D:/OneDrive/OneDrive - Universiteit Antwerpen/R projects/GIT/CurieuzeNeuzen-in-de-Tuin/data/database/all_2021-04-09_2021-06-27_tzUTCplus2.csv"

  # declare test functions
base.read.csv <- function() read.csv(path)
readr.read_csv.old <- function() readr::with_edition(1, readr::read_csv(path))
readr.read_csv.new <- function() readr::read_csv(path)
dt.fread <- function() data.table::fread(path)

microbenchmark::microbenchmark(
  base.read.csv(), 
  readr.read_csv.old(),
  readr.read_csv.new(), 
  dt.fread(),
  times = 5)
```

```{r comparison, eval = FALSE}
Unit: seconds
                 expr        min         lq       mean     median         uq        max neval
      base.read.csv() 111.372018 113.808849 114.516900 113.859472 116.155927 117.388234     5
 readr.read_csv.old()  37.959150  38.016099  38.741874  38.960399  39.156582  39.617139     5
 readr.read_csv.new()   4.326258   4.599566   4.941215   4.649826   4.681539   6.448888     5
           dt.fread()   3.465170   3.667079   3.716254   3.742631   3.815739   3.890653     5
```

There are ways to optimize functions (such as `read.table()`) to handle data more efficiently, but this is very situational and far beyond the scope of this workshop.

To find out more about [vroom](https://vroom.r-lib.org/) and [data.table](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html), click on the embedded links.

## 5. Data manipulation with `dplyr`
Once you've loaded your data into R, it's time to start tinkering! A package that aims to make handling and manipulating data easier and more efficient is `dplyr`(pronounced d-ply-r), which you'll use most often when it comes to data manipulation. It presents itself as a *grammar of data manipulation*, providing a couple of functions to solve most common data manipulation challenges, also referred to as **verbs**: `select()`, `filter()`, `mutate()`, `arrange()`, etc. 

These so-called verbs can be used together harmoniously, aiming to make data cleaning a much smoother and readable experience. For your reference, I will list the functions used in **[5.10 Overview]**, each accompanied by a short description. Worked out examples will be provided in the next sections. Similar examples can be found in the package's vignette by using `vignette("dplyr")` in R. If you are curious how to perform these operations in base R, please refer to following [dplyr<->base R](https://dplyr.tidyverse.org/articles/base.html) article.

Using one of the datasets included with `dplyr`, we will explore the world of Star Wars. As you can see below, we have 14 variables (columns) with 87 observations (rows), so we have a lot to work with! The `glimpse()` function is included in the `dplyr` package (but is actually exported from `tibble`, which, in turn, exported it from `pillar` - phew!), and gives us a 'glimpse' of the dataset.
```{r head star wars, collapse=TRUE}
glimpse(starwars, width = 75)

```
### 5.1 `select()`
Often, datasets will contain a lot of information you don't need. For this purpose, it can be useful to keep only those columns you are using in further analyses.

To select a column, simply type the names of the columns you want to keep. Conversely, the same can be achieved by writing the column names you don't need, preceded by the `-` operator. Compare the pieces of code below, as well as the results.

```{r select1, eval = FALSE}
  # select columns name, height, mass, homeworld and species
starwars %>% 
  select(name, height, mass, homeworld, species)

starwars %>% 
  select(-hair_color, -skin_color, -eye_color, -birth_year, -sex, -gender, -films, -vehicles, -starships)

  # base R solution+
starwars[c("name", "height", "mass", "homeworld", "species")]
subset(starwars, select = c(name, height, mass, homeworld, species))
```

You may be wondering whether there aren't any more efficient ways to select columns, rather than typing out each individual one. Fortunately, there are multiple ways to achieve the same result! In a lot of cases, the `:` operator can be used to select adjacent columns (`FROM:TO`), as shown below.
```{r select2, eval = FALSE}
starwars %>% 
  select(name:mass, homeworld, species)

starwars %>% 
  select(-c(hair_color:gender, films:starships))

```

You can even select columns without ever mentioning any column names, using one of many **selection helpers**; see `?tidyselect::language` for a list of all `tidyselect` helper functions. Below, I will show `where()` and `contains()`.

```{r select3, eval = FALSE}
  # select all numeric columns
starwars %>% 
  select(where(is.numeric))

  # select all columns that contain the letter 'o'
starwars %>% 
  select(contains("o"))
```

### 5.2 `filter()`
Filtering can be performed for many reasons. Perhaps you are only interested in Star Wars characters with a certain hair or skin colour, a certain species, or characters who are at least as tall as [the expected average height of a healthy population as defined by the WHO growth reference standards](https://ourworldindata.org/human-height). Whatever your flavour may be, all of this can be done using `filter()` to filter specific rows. 

```{r filter1a, collapse = TRUE}
  # characters with a gold skin colour
starwars %>% 
  filter(skin_color == "gold")

```
```{r filter1b, eval = FALSE, echo = FALSE}
  # the same results can be achieved without %>%
filter(starwars, skin_color == "gold")

  # base R solutions
starwars[starwars$skin_color == "gold", , drop = FALSE]
subset(starwars, skin_color == "gold")
```

If we want to take it a step further, we can also combine different statements together. Try running the following code:

```{r filter2, eval = FALSE}
  # return only characters that are masculine, not with a golden skin color and at least 176.5 cm tall
starwars %>% 
  filter(gender == "masculine", 
         skin_color != "gold", 
         height >= 176.5)
```

Any of the base R relational operators (`?Comparison`) can be used in conjunction with `filter()`. By default, the `,` acts as the `&` operator within a `filter()` call.

We can also provide a vector to filter a column, for which the `%in%` operator is used. In conjunction with the `!` operator (preceding the column name you are filtering on), you can also exclude parts of your dataset. Try running the code below and spot the differences.

```{r filter3, eval = FALSE}
  # characters with eye_color == "blue" OR eye_color == "red"
starwars %>% 
  filter(eye_color %in% c("blue", "red"))

  # characters NOT with eye_color == "blue" OR eye_color == "red
starwars %>% 
  filter(!eye_color %in% c("blue", "red"))
```

However, there are also a couple of rows that contain multiple colours, as is shown by `unique(starwars$eye_color)`, meaning our current filtering methods are not completely waterproof. Without going into detail, the `stringr` package provides a solution to overcome this issue (e.g. `str_detect()`). Run the following code to confirm that, indeed, all characters with red eyes (even if the character has multiple eye colours) are returned.

```{r filter4, eval = FALSE}
starwars %>% 
  filter(str_detect(string = eye_color, pattern = "red"))
```

### 5.3 `arrange()`
While `filter()` effectively selects or omits certain rows, `arrange()` simply reorders them. By default, it arranges rows in an ascending order. In case of character columns, they are reordered alphabetically (A-Z), while numeric columns will be reorderd from smallest to largest numbers.
For the record, `arrange()` reorders the *entire* dataframe according to the column you have selected.
```{r arrange1, eval = FALSE}
  # reordering a character column
starwars %>% 
  arrange(name)

  # reordering a numeric column
starwars %>% 
  arrange(height)

```

If we want to reorder rows in a descending order, then the helper function `desc()` can be used.
```{r arrange2, eval = FALSE}
starwars %>% 
  arrange(desc(height))
```

Furthermore, multiple columns can be used for reordering. For completion's sake, I will also provide the base R equivalents.

**WORK IN PROGRESS**

```{r arrange3}
  # base R: arranging in ascending order
mtcars[order(mtcars$cyl, mtcars$disp), , drop = FALSE]

  # base R: arranging in descending order
mtcars[order(mtcars$cyl, mtcars$disp, decreasing = TRUE), , drop = FALSE]
mtcars[order(-mtcars$cyl, -mtcars$disp), , drop = FALSE]

```


### 5.4 `distinct()`

### 5.5 `mutate()`

### 5.6 `group_by()`

### 5.7 `summarise()`

### 5.8 mutating joins

### 5.9 filtering joins

### 5.10 Overview

* `filter()` subsets rows using column values
* `select()` subsets columns (and optionally renames) using their names
* `arrange()` arrange rows by column values
* `distinct()` selects only unique/distinct rows from a data frame
* `mutate()` adds new variables that are functions of existing variables
* `group_by()` applies grouping by one (or more) variables (this doesn't change how the data looks, but changes how it acts with other `dplyr` verbs)
* `ungroup()` complements `group_by()` by removing a layer of grouping
* `summarise()/summarize()` reduces multiple values down to a single summary (removes a layer of grouping)
* **mutating joins** add columns from dataset `y` to dataset `x` (see ?`filter-joins` more information)
* **filtering joins** filter rows from `x` based on the presence/absence of matches in `y` (see any of these joins' documentation for more information; `?*_join`)

### 5.11 Exercises
Below you will find a couple of exercises to practice the `dplyr` verbs above. Often, multiple solutions exist to obtain the same answer. Feel free to use whichever tools you have at your disposal, but see to it that you *also* try with the things shown in this section!

#### Exercise 1
* Using the `starwars` dataset, select all character columns, along with `height`. Bonus points if you can come up with multiple ways to achieve this!
* With the selected columns, find all characters that are:
    + taller than 150 cm and have a maximum height of 210 cm
    + have blond or no hair colour ("none")
    + are of the Human species
    + not born on planets Bespin, Coruscant or Haruun Kal

```{r ex1 solution, eval = FALSE, echo = FALSE}
starwars %>% 
  select(height, where(is.character)) %>% 
  filter(height > 150, height <= 210, 
         hair_color == "blond" | hair_color == "none",
         species == "Human", 
         !homeworld %in% c("Bespin", "Coruscant", "Haruun Kal")
         )
```

#### Exercise 2




## 6. `tidyr`
`tidyr` (prounounced tidy-r) was designed to allow reshaping and pivoting data. For instance, your raw data may be formatted as a wide table, while you want this dataset to be formatted as a long table. The long format, in particular, is *usually* preferred when working with `tidyverse`. As most built-in R functions work with vectors of values, it is only natural that `tidyverse` would follow suit. 

filter
select
arrange
distinct
mutate
summarise group_by
n
sample_n

pivoting: WorldPhones or co2




### 6.1 select()
### 6.2 filter()
rename()
glipse()
mutate()
group_by() and summarize()
arrange()
count()
across()
joining tables
coalesce()
distinct()
if_else and case_when


## 7. What's next?
visualization (if not modelling)

## 8. Common errors
ggplot + vs %>% 
masking functions (e.g. dplyr select)

## 9. Life cycle

## 10. Cheat sheets
[dplyr](https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf)

## Session Info
```{r sessioninfo, echo = FALSE}
sessionInfo()
```

